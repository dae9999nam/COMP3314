{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, transforms\n",
    "import time\n",
    "import os\n",
    "\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Input - 1x32x32 (Gray image)\n",
    "    Output - 10 (Number for classification)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Flatten(),  # flatten the image\n",
    "            nn.Linear(32 * 32 * 3, 512),  # fully connected layers\n",
    "            # nn.Linear(32 * 32 * 3, 512) for color image handling\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),          # dropout for overfitting\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)\n",
    "\n",
    "\"\"\"\n",
    "# 3) ARCHITECTURE TUNING\n",
    "# ==================================================================\n",
    "\n",
    "\n",
    "class NetTuned(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(32*32*1, 512),\n",
    "            nn.BatchNorm1d(512),         # internal covariate shift reduction\n",
    "            nn.LeakyReLU(0.1),           # nonzero gradient for x<0\n",
    "            nn.Dropout(0.5),             # regularization\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.4),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(128, 10)           # logits for 10 classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# In main:\n",
    "model_ft = NetTuned().to(device)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can try different types of data augementation to increase the performance on test data.\n",
    "#Original Data Augmentation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),  \n",
    "        transforms.RandomAffine(degrees=10, translate=(0,0.1)),\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])  # normalization\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]),\n",
    "}\n",
    "\"\"\"\n",
    "# 2) DATA AUGMENTATION TUNING\n",
    "# ==================================================================\n",
    "# For different Data Augmentation, now in color\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(32, scale=(0.8, 1.0)),  # random crop\n",
    "        transforms.RandomHorizontalFlip(p=0.5),              # flip\n",
    "        transforms.ColorJitter(                               \n",
    "            brightness=0.2, contrast=0.2, saturation=0.2\n",
    "        ),                                                    # color jitter\n",
    "        transforms.RandomAffine(degrees=10, translate=(0.1,0.1)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    train_loss = []\n",
    "    train_accuracy = []\n",
    "    history = {}\n",
    "\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects += (preds == labels).sum().item()\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}] Iter [{i+1}] loss: {running_loss/((i+1)*inputs.size(0)):.3f}\")\n",
    "\n",
    "        epoch_loss = running_loss / train_size\n",
    "        epoch_acc = running_corrects / train_size * 100\n",
    "        print(f\"Epoch [{epoch+1}] Training Acc: {epoch_acc:.2f}% Loss: {epoch_loss:.3f}\")\n",
    "        train_loss.append(epoch_loss)\n",
    "        train_accuracy.append(epoch_acc)\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch time: {time.time() - start_time:.1f}s\")\n",
    "\n",
    "    history['train_loss'] = train_loss\n",
    "    history['train_accuracy'] = train_accuracy\n",
    "\n",
    "    # Evaluation phase with overall & per-class accuracy\n",
    "    model.eval()\n",
    "    num_classes = len(image_datasets['test'].classes)\n",
    "    class_correct = [0] * num_classes\n",
    "    class_total = [0] * num_classes\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "\n",
    "            for t, p in zip(labels, preds):\n",
    "                class_total[t.item()] += 1\n",
    "                class_correct[t.item()] += int(p == t)\n",
    "\n",
    "    overall_acc = total_correct / total_samples * 100\n",
    "    print(f\"Test Accuracy: {overall_acc:.2f}%\")\n",
    "\n",
    "    print(\"\\nPer-class Accuracy:\")\n",
    "    for idx, cls in enumerate(image_datasets['test'].classes):\n",
    "        acc = class_correct[idx] / class_total[idx] * 100\n",
    "        print(f\"  Class {cls:>2s}: {acc:.2f}% ({class_correct[idx]}/{class_total[idx]})\")\n",
    "\n",
    "    history['val_accuracy'] = overall_acc\n",
    "    history['per_class'] = {cls: class_correct[i] / class_total[i] * 100\n",
    "                            for i, cls in enumerate(image_datasets['test'].classes)}\n",
    "\n",
    "    return history, overall_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  (1): Linear(in_features=3072, out_features=512, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Dropout(p=0.5, inplace=False)\n",
      "  (4): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Dropout(p=0.3, inplace=False)\n",
      "  (7): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "Epoch [1] Iter [100] loss: 2.341\n",
      "Epoch [1] Iter [200] loss: 2.323\n",
      "Epoch [1] Training Acc: 11.18% Loss: 2.319\n",
      "Epoch time: 46.0s\n",
      "Epoch [2] Iter [100] loss: 2.268\n",
      "Epoch [2] Iter [200] loss: 2.250\n",
      "Epoch [2] Training Acc: 16.23% Loss: 2.243\n",
      "Epoch time: 43.5s\n",
      "Epoch [3] Iter [100] loss: 2.184\n",
      "Epoch [3] Iter [200] loss: 2.172\n",
      "Epoch [3] Training Acc: 20.23% Loss: 2.168\n",
      "Epoch time: 46.6s\n",
      "Epoch [4] Iter [100] loss: 2.108\n",
      "Epoch [4] Iter [200] loss: 2.098\n",
      "Epoch [4] Training Acc: 24.26% Loss: 2.095\n",
      "Epoch time: 46.0s\n",
      "Epoch [5] Iter [100] loss: 2.066\n",
      "Epoch [5] Iter [200] loss: 2.050\n",
      "Epoch [5] Training Acc: 26.58% Loss: 2.043\n",
      "Epoch time: 43.7s\n",
      "Epoch [6] Iter [100] loss: 2.012\n",
      "Epoch [6] Iter [200] loss: 1.995\n",
      "Epoch [6] Training Acc: 29.44% Loss: 1.990\n",
      "Epoch time: 45.9s\n",
      "Epoch [7] Iter [100] loss: 1.966\n",
      "Epoch [7] Iter [200] loss: 1.952\n",
      "Epoch [7] Training Acc: 31.25% Loss: 1.950\n",
      "Epoch time: 46.6s\n",
      "Epoch [8] Iter [100] loss: 1.928\n",
      "Epoch [8] Iter [200] loss: 1.914\n",
      "Epoch [8] Training Acc: 33.23% Loss: 1.912\n",
      "Epoch time: 43.1s\n",
      "Epoch [9] Iter [100] loss: 1.883\n",
      "Epoch [9] Iter [200] loss: 1.883\n",
      "Epoch [9] Training Acc: 34.32% Loss: 1.881\n",
      "Epoch time: 43.7s\n",
      "Epoch [10] Iter [100] loss: 1.862\n",
      "Epoch [10] Iter [200] loss: 1.855\n",
      "Epoch [10] Training Acc: 35.76% Loss: 1.850\n",
      "Epoch time: 44.7s\n",
      "Epoch [11] Iter [100] loss: 1.829\n",
      "Epoch [11] Iter [200] loss: 1.825\n",
      "Epoch [11] Training Acc: 36.96% Loss: 1.821\n",
      "Epoch time: 44.9s\n",
      "Epoch [12] Iter [100] loss: 1.796\n",
      "Epoch [12] Iter [200] loss: 1.796\n",
      "Epoch [12] Training Acc: 37.43% Loss: 1.797\n",
      "Epoch time: 44.4s\n",
      "Epoch [13] Iter [100] loss: 1.782\n",
      "Epoch [13] Iter [200] loss: 1.773\n",
      "Epoch [13] Training Acc: 38.38% Loss: 1.774\n",
      "Epoch time: 44.0s\n",
      "Epoch [14] Iter [100] loss: 1.764\n",
      "Epoch [14] Iter [200] loss: 1.759\n",
      "Epoch [14] Training Acc: 39.24% Loss: 1.759\n",
      "Epoch time: 43.5s\n",
      "Epoch [15] Iter [100] loss: 1.744\n",
      "Epoch [15] Iter [200] loss: 1.741\n",
      "Epoch [15] Training Acc: 39.91% Loss: 1.740\n",
      "Epoch time: 44.5s\n",
      "Epoch [16] Iter [100] loss: 1.740\n",
      "Epoch [16] Iter [200] loss: 1.730\n",
      "Epoch [16] Training Acc: 40.51% Loss: 1.727\n",
      "Epoch time: 47.9s\n",
      "Epoch [17] Iter [100] loss: 1.705\n",
      "Epoch [17] Iter [200] loss: 1.696\n",
      "Epoch [17] Training Acc: 42.08% Loss: 1.695\n",
      "Epoch time: 47.9s\n",
      "Epoch [18] Iter [100] loss: 1.691\n",
      "Epoch [18] Iter [200] loss: 1.680\n",
      "Epoch [18] Training Acc: 42.00% Loss: 1.681\n",
      "Epoch time: 43.9s\n",
      "Epoch [19] Iter [100] loss: 1.661\n",
      "Epoch [19] Iter [200] loss: 1.665\n",
      "Epoch [19] Training Acc: 42.95% Loss: 1.665\n",
      "Epoch time: 42.0s\n",
      "Epoch [20] Iter [100] loss: 1.658\n",
      "Epoch [20] Iter [200] loss: 1.659\n",
      "Epoch [20] Training Acc: 43.65% Loss: 1.656\n",
      "Epoch time: 45.5s\n",
      "Epoch [21] Iter [100] loss: 1.656\n",
      "Epoch [21] Iter [200] loss: 1.642\n",
      "Epoch [21] Training Acc: 43.80% Loss: 1.645\n",
      "Epoch time: 43.6s\n",
      "Epoch [22] Iter [100] loss: 1.643\n",
      "Epoch [22] Iter [200] loss: 1.638\n",
      "Epoch [22] Training Acc: 44.65% Loss: 1.636\n",
      "Epoch time: 41.9s\n",
      "Epoch [23] Iter [100] loss: 1.632\n",
      "Epoch [23] Iter [200] loss: 1.639\n",
      "Epoch [23] Training Acc: 44.12% Loss: 1.636\n",
      "Epoch time: 43.5s\n",
      "Epoch [24] Iter [100] loss: 1.619\n",
      "Epoch [24] Iter [200] loss: 1.617\n",
      "Epoch [24] Training Acc: 45.17% Loss: 1.612\n",
      "Epoch time: 46.3s\n",
      "Epoch [25] Iter [100] loss: 1.601\n",
      "Epoch [25] Iter [200] loss: 1.611\n",
      "Epoch [25] Training Acc: 45.58% Loss: 1.612\n",
      "Epoch time: 43.6s\n",
      "Epoch [26] Iter [100] loss: 1.613\n",
      "Epoch [26] Iter [200] loss: 1.607\n",
      "Epoch [26] Training Acc: 45.57% Loss: 1.605\n",
      "Epoch time: 43.8s\n",
      "Epoch [27] Iter [100] loss: 1.608\n",
      "Epoch [27] Iter [200] loss: 1.591\n",
      "Epoch [27] Training Acc: 46.27% Loss: 1.587\n",
      "Epoch time: 44.7s\n",
      "Epoch [28] Iter [100] loss: 1.582\n",
      "Epoch [28] Iter [200] loss: 1.585\n",
      "Epoch [28] Training Acc: 46.36% Loss: 1.584\n",
      "Epoch time: 45.6s\n",
      "Epoch [29] Iter [100] loss: 1.582\n",
      "Epoch [29] Iter [200] loss: 1.566\n",
      "Epoch [29] Training Acc: 46.84% Loss: 1.568\n",
      "Epoch time: 45.4s\n",
      "Epoch [30] Iter [100] loss: 1.556\n",
      "Epoch [30] Iter [200] loss: 1.556\n",
      "Epoch [30] Training Acc: 47.57% Loss: 1.554\n",
      "Epoch time: 44.5s\n",
      "Epoch [31] Iter [100] loss: 1.546\n",
      "Epoch [31] Iter [200] loss: 1.547\n",
      "Epoch [31] Training Acc: 48.16% Loss: 1.550\n",
      "Epoch time: 45.0s\n",
      "Epoch [32] Iter [100] loss: 1.546\n",
      "Epoch [32] Iter [200] loss: 1.542\n",
      "Epoch [32] Training Acc: 48.05% Loss: 1.541\n",
      "Epoch time: 43.8s\n",
      "Epoch [33] Iter [100] loss: 1.540\n",
      "Epoch [33] Iter [200] loss: 1.543\n",
      "Epoch [33] Training Acc: 48.31% Loss: 1.536\n",
      "Epoch time: 44.1s\n",
      "Epoch [34] Iter [100] loss: 1.521\n",
      "Epoch [34] Iter [200] loss: 1.514\n",
      "Epoch [34] Training Acc: 48.90% Loss: 1.517\n",
      "Epoch time: 44.0s\n",
      "Epoch [35] Iter [100] loss: 1.524\n",
      "Epoch [35] Iter [200] loss: 1.525\n",
      "Epoch [35] Training Acc: 48.34% Loss: 1.527\n",
      "Epoch time: 43.4s\n",
      "Epoch [36] Iter [100] loss: 1.507\n",
      "Epoch [36] Iter [200] loss: 1.513\n",
      "Epoch [36] Training Acc: 49.35% Loss: 1.511\n",
      "Epoch time: 45.9s\n",
      "Epoch [37] Iter [100] loss: 1.514\n",
      "Epoch [37] Iter [200] loss: 1.516\n",
      "Epoch [37] Training Acc: 49.07% Loss: 1.514\n",
      "Epoch time: 44.5s\n",
      "Epoch [38] Iter [100] loss: 1.504\n",
      "Epoch [38] Iter [200] loss: 1.504\n",
      "Epoch [38] Training Acc: 49.86% Loss: 1.504\n",
      "Epoch time: 44.8s\n",
      "Epoch [39] Iter [100] loss: 1.496\n",
      "Epoch [39] Iter [200] loss: 1.486\n",
      "Epoch [39] Training Acc: 50.40% Loss: 1.486\n",
      "Epoch time: 43.3s\n",
      "Epoch [40] Iter [100] loss: 1.495\n",
      "Epoch [40] Iter [200] loss: 1.501\n",
      "Epoch [40] Training Acc: 50.00% Loss: 1.503\n",
      "Epoch time: 42.7s\n",
      "Epoch [41] Iter [100] loss: 1.494\n",
      "Epoch [41] Iter [200] loss: 1.490\n",
      "Epoch [41] Training Acc: 50.11% Loss: 1.494\n",
      "Epoch time: 42.7s\n",
      "Epoch [42] Iter [100] loss: 1.474\n",
      "Epoch [42] Iter [200] loss: 1.470\n",
      "Epoch [42] Training Acc: 50.88% Loss: 1.470\n",
      "Epoch time: 42.9s\n",
      "Epoch [43] Iter [100] loss: 1.484\n",
      "Epoch [43] Iter [200] loss: 1.488\n",
      "Epoch [43] Training Acc: 50.45% Loss: 1.486\n",
      "Epoch time: 41.9s\n",
      "Epoch [44] Iter [100] loss: 1.484\n",
      "Epoch [44] Iter [200] loss: 1.481\n",
      "Epoch [44] Training Acc: 50.44% Loss: 1.485\n",
      "Epoch time: 41.9s\n",
      "Epoch [45] Iter [100] loss: 1.468\n",
      "Epoch [45] Iter [200] loss: 1.472\n",
      "Epoch [45] Training Acc: 51.15% Loss: 1.471\n",
      "Epoch time: 41.7s\n",
      "Epoch [46] Iter [100] loss: 1.478\n",
      "Epoch [46] Iter [200] loss: 1.465\n",
      "Epoch [46] Training Acc: 51.05% Loss: 1.465\n",
      "Epoch time: 45.6s\n",
      "Epoch [47] Iter [100] loss: 1.441\n",
      "Epoch [47] Iter [200] loss: 1.438\n",
      "Epoch [47] Training Acc: 52.11% Loss: 1.442\n",
      "Epoch time: 42.1s\n",
      "Epoch [48] Iter [100] loss: 1.441\n",
      "Epoch [48] Iter [200] loss: 1.440\n",
      "Epoch [48] Training Acc: 51.83% Loss: 1.444\n",
      "Epoch time: 42.9s\n",
      "Epoch [49] Iter [100] loss: 1.433\n",
      "Epoch [49] Iter [200] loss: 1.447\n",
      "Epoch [49] Training Acc: 51.93% Loss: 1.445\n",
      "Epoch time: 43.7s\n",
      "Epoch [50] Iter [100] loss: 1.431\n",
      "Epoch [50] Iter [200] loss: 1.445\n",
      "Epoch [50] Training Acc: 52.20% Loss: 1.435\n",
      "Epoch time: 42.7s\n",
      "Test Accuracy: 61.82%\n",
      "\n",
      "Per-class Accuracy:\n",
      "  Class  0: 65.00% (325/500)\n",
      "  Class  1: 82.40% (412/500)\n",
      "  Class  2: 60.40% (302/500)\n",
      "  Class  3: 54.80% (274/500)\n",
      "  Class  4: 74.20% (371/500)\n",
      "  Class  5: 58.80% (294/500)\n",
      "  Class  6: 48.80% (244/500)\n",
      "  Class  7: 67.40% (337/500)\n",
      "  Class  8: 40.60% (203/500)\n",
      "  Class  9: 65.80% (329/500)\n",
      "time required 2242.99s\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # change the data-path, recommand for relative path\n",
    "    data_dir = './data'  # change with the true parh\n",
    "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                            data_transforms[x])\n",
    "                    for x in ['train', 'test']}\n",
    "\n",
    "    data_dir = './data' # Suppose the dataset is stored under this folder\n",
    "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                            data_transforms[x])\n",
    "                    for x in ['train', 'test']} # Read train and test sets, respectively.\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(image_datasets['train'], batch_size=128,\n",
    "                                                shuffle=True, num_workers=4)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(image_datasets['test'], batch_size=128,\n",
    "                                                shuffle=False, num_workers=4)\n",
    "\n",
    "    train_size =len(image_datasets['train'])\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Set device to \"cpu\" if you have no gpu\n",
    "\n",
    "    end = time.time()\n",
    "    model_ft = Net().to(device)\n",
    "    print(model_ft.network)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # paramters for optimizer\n",
    "    optimizer_ft = optim.AdamW(\n",
    "    model_ft.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-2\n",
    "    )\n",
    "    # Tuning Optimizer\n",
    "    \"\"\"\n",
    "    # 1) OPTIMIZER TUNING\n",
    "    # ==================================================================\n",
    "    # Original paramters for optimizer\n",
    "    optimizer_ft = optim.Adam(model_ft.parameters(), lr=1e-3)  \"\"\"\n",
    "    \n",
    "    \"\"\" SGD with Momentum\n",
    "    optimizer_ft = optim.SGD(\n",
    "    model_ft.parameters(),\n",
    "    lr=1e-3,\n",
    "    momentum=0.9,\n",
    "    ) \n",
    "    # AdamW\n",
    "    optimizer = optim.AdamW(\n",
    "    model_ft.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-2\n",
    "    )\"\"\"\n",
    "\n",
    "\n",
    "    # learning rate scheduler\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=15, gamma=0.9)\n",
    "    \n",
    "    # learning epoch\n",
    "    history, accuracy = train_test(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "               num_epochs=50)\n",
    "    \n",
    "    print(\"time required %.2fs\" %(time.time() - end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) LR DECAY STRATEGY TUNING\n",
    "# ==================================================================\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# (a) Cosine annealing over full training\n",
    "optimizer = optim.Adam(model_ft.parameters(), lr=1e-3)\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "hist_cos, acc_cos = train_test(model_ft, criterion, optimizer, scheduler, num_epochs=50)\n",
    "print(f\"CosineAnnealing → {acc_cos:.4f}\")\n",
    "\n",
    "# (b) Exponential decay with warm‐up\n",
    "class WarmupExpLR(lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, warmup_epochs=5, gamma=0.95, last_epoch=-1):\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.gamma = gamma\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.warmup_epochs:\n",
    "            # ramp from 0.1× to 1.0× over warmup\n",
    "            return [\n",
    "                base_lr * (0.1 + 0.9 * self.last_epoch / self.warmup_epochs)\n",
    "                for base_lr in self.base_lrs\n",
    "            ]\n",
    "        # then exponential decay\n",
    "        return [group['lr'] * self.gamma for group in self.optimizer.param_groups]\n",
    "\n",
    "scheduler = WarmupExpLR(optimizer, warmup_epochs=5, gamma=0.97)\n",
    "hist_we, acc_we = train_test(model_ft, criterion, optimizer, scheduler, num_epochs=50)\n",
    "print(f\"Warmup+ExpDecay → {acc_we:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
